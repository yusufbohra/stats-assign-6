{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "268f420d-deed-4a15-8f78-0ede2158ef8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Analysis of Variance (ANOVA) is a statistical technique used to compare means among more than two groups. It helps determine if there are any statistically significant differences between the means of these groups. To ensure the validity of ANOVA results, certain assumptions must be met. These assumptions include:\\n\\n1. **Independence of Observations:** The observations within each group are assumed to be independent of each other. This means that the data points in one group should not be influenced by or related to the data points in another group.\\n\\n2. **Normality:** The residuals (the differences between the observed values and the group means) should be approximately normally distributed within each group. Normality ensures that the sampling distribution of the mean is also normal, which is important for accurate hypothesis testing.\\n\\n3. **Homogeneity of Variance (Homoscedasticity):** The variance of the residuals should be roughly equal across all groups. This means that the spread of data points around the group means should be similar for all groups.\\n\\nNow, let's discuss examples of violations for each assumption:\\n\\n1. **Independence of Observations:**\\n   - Example Violation: In a study of employee performance in different departments of a company, the performance ratings within each department are heavily influenced by the overall departmental culture, which is not independent across different departments.\\n\\n2. **Normality:**\\n   - Example Violation: In a study comparing test scores of students from different schools, the test scores in each school are heavily skewed due to various factors. For instance, the scores may be skewed to the right (positively skewed) due to a group of high-performing students.\\n\\n3. **Homogeneity of Variance:**\\n   - Example Violation: In an experiment comparing the yields of crops grown using different fertilizers, the variance in crop yields for one type of fertilizer is much larger than the variance for the others. This unequal spread of data points around the means can violate the homogeneity of variance assumption.\\n\\nWhen these assumptions are violated, the validity of the ANOVA results can be compromised. Violations can lead to incorrect conclusions, such as:\\n- False positives (Type I errors) or false negatives (Type II errors) in hypothesis testing.\\n- Incorrectly identifying significant differences or failing to detect significant differences when they do exist.\\n- Biased estimates of treatment effects.\\n\\nIn cases where assumptions are not met, alternative statistical methods or transformations of the data might be necessary. It's important to assess the assumptions through graphical methods (like residual plots and Q-Q plots) and formal statistical tests (like Shapiro-Wilk test for normality and Levene's test for homogeneity of variance) before relying on ANOVA results. If assumptions are severely violated, non-parametric tests or more specialized techniques might be more appropriate.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#que 1\n",
    "\"\"\"Analysis of Variance (ANOVA) is a statistical technique used to compare means among more than two groups. It helps determine if there are any statistically significant differences between the means of these groups. To ensure the validity of ANOVA results, certain assumptions must be met. These assumptions include:\n",
    "\n",
    "1. **Independence of Observations:** The observations within each group are assumed to be independent of each other. This means that the data points in one group should not be influenced by or related to the data points in another group.\n",
    "\n",
    "2. **Normality:** The residuals (the differences between the observed values and the group means) should be approximately normally distributed within each group. Normality ensures that the sampling distribution of the mean is also normal, which is important for accurate hypothesis testing.\n",
    "\n",
    "3. **Homogeneity of Variance (Homoscedasticity):** The variance of the residuals should be roughly equal across all groups. This means that the spread of data points around the group means should be similar for all groups.\n",
    "\n",
    "Now, let's discuss examples of violations for each assumption:\n",
    "\n",
    "1. **Independence of Observations:**\n",
    "   - Example Violation: In a study of employee performance in different departments of a company, the performance ratings within each department are heavily influenced by the overall departmental culture, which is not independent across different departments.\n",
    "\n",
    "2. **Normality:**\n",
    "   - Example Violation: In a study comparing test scores of students from different schools, the test scores in each school are heavily skewed due to various factors. For instance, the scores may be skewed to the right (positively skewed) due to a group of high-performing students.\n",
    "\n",
    "3. **Homogeneity of Variance:**\n",
    "   - Example Violation: In an experiment comparing the yields of crops grown using different fertilizers, the variance in crop yields for one type of fertilizer is much larger than the variance for the others. This unequal spread of data points around the means can violate the homogeneity of variance assumption.\n",
    "\n",
    "When these assumptions are violated, the validity of the ANOVA results can be compromised. Violations can lead to incorrect conclusions, such as:\n",
    "- False positives (Type I errors) or false negatives (Type II errors) in hypothesis testing.\n",
    "- Incorrectly identifying significant differences or failing to detect significant differences when they do exist.\n",
    "- Biased estimates of treatment effects.\n",
    "\n",
    "In cases where assumptions are not met, alternative statistical methods or transformations of the data might be necessary. It's important to assess the assumptions through graphical methods (like residual plots and Q-Q plots) and formal statistical tests (like Shapiro-Wilk test for normality and Levene's test for homogeneity of variance) before relying on ANOVA results. If assumptions are severely violated, non-parametric tests or more specialized techniques might be more appropriate.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e55f6071-5769-4407-971e-eb36c7b41221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Analysis of Variance (ANOVA) can be categorized into three main types based on the number of factors or independent variables involved and the design of the study. These three types are:\\n\\n1. **One-Way ANOVA:**\\n   - **Number of Factors:** One\\n   - **Situations:** One-Way ANOVA is used when you have one categorical independent variable (also called a factor) and one continuous dependent variable. It's used to test if there are any significant differences in the means of the dependent variable across different levels or groups of the independent variable.\\n   - **Example:** A researcher wants to compare the test scores of students from three different schools to see if there are any significant differences in the mean scores among the schools.\\n\\n2. **Two-Way ANOVA:**\\n   - **Number of Factors:** Two\\n   - **Situations:** Two-Way ANOVA is used when you have two categorical independent variables (factors) and one continuous dependent variable. It's used to examine the interaction effects between the two independent variables and their individual effects on the dependent variable.\\n   - **Example:** A study investigates the effects of two different factors, such as diet and exercise regimen, on the weight loss of individuals. The study aims to determine if the effect of diet depends on the level of exercise, and vice versa.\\n\\n3. **N-Way ANOVA (Factorial ANOVA):**\\n   - **Number of Factors:** More than two\\n   - **Situations:** N-Way ANOVA is an extension of Two-Way ANOVA and is used when you have more than two categorical independent variables (factors) and a continuous dependent variable. It allows you to examine the effects of multiple factors on the dependent variable and their interactions.\\n   - **Example:** A study examines how factors like age group, gender, and education level collectively influence job satisfaction scores among employees.\\n\\nIn each type of ANOVA, the goal is to assess whether there are statistically significant differences in means among different groups or levels of the categorical independent variables. The choice of ANOVA type depends on the research question and the experimental design of the study. It's important to select the appropriate type of ANOVA to ensure accurate and meaningful statistical analysis.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#que 2\n",
    "\"\"\"Analysis of Variance (ANOVA) can be categorized into three main types based on the number of factors or independent variables involved and the design of the study. These three types are:\n",
    "\n",
    "1. **One-Way ANOVA:**\n",
    "   - **Number of Factors:** One\n",
    "   - **Situations:** One-Way ANOVA is used when you have one categorical independent variable (also called a factor) and one continuous dependent variable. It's used to test if there are any significant differences in the means of the dependent variable across different levels or groups of the independent variable.\n",
    "   - **Example:** A researcher wants to compare the test scores of students from three different schools to see if there are any significant differences in the mean scores among the schools.\n",
    "\n",
    "2. **Two-Way ANOVA:**\n",
    "   - **Number of Factors:** Two\n",
    "   - **Situations:** Two-Way ANOVA is used when you have two categorical independent variables (factors) and one continuous dependent variable. It's used to examine the interaction effects between the two independent variables and their individual effects on the dependent variable.\n",
    "   - **Example:** A study investigates the effects of two different factors, such as diet and exercise regimen, on the weight loss of individuals. The study aims to determine if the effect of diet depends on the level of exercise, and vice versa.\n",
    "\n",
    "3. **N-Way ANOVA (Factorial ANOVA):**\n",
    "   - **Number of Factors:** More than two\n",
    "   - **Situations:** N-Way ANOVA is an extension of Two-Way ANOVA and is used when you have more than two categorical independent variables (factors) and a continuous dependent variable. It allows you to examine the effects of multiple factors on the dependent variable and their interactions.\n",
    "   - **Example:** A study examines how factors like age group, gender, and education level collectively influence job satisfaction scores among employees.\n",
    "\n",
    "In each type of ANOVA, the goal is to assess whether there are statistically significant differences in means among different groups or levels of the categorical independent variables. The choice of ANOVA type depends on the research question and the experimental design of the study. It's important to select the appropriate type of ANOVA to ensure accurate and meaningful statistical analysis.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef69d37-6cfc-4be6-bab6-6a6b336981bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Partitioning of variance is a fundamental concept in Analysis of Variance (ANOVA) that involves breaking down the total variability in a dataset into different components that can be attributed to different sources of variation. The goal of partitioning variance is to understand how much of the total variation in the dependent variable can be explained by the factors being studied and how much is due to random or unexplained variability. This breakdown provides insights into the contributions of different factors and interactions between them, helping researchers draw meaningful conclusions about the significance of those factors.\\n\\nIn ANOVA, the total variance is partitioned into two main components:\\n\\n1. **Between-Groups Variance (Between-Treatments Variance):** This component of variance measures the variation among the group means. It quantifies how much the means of different groups (treatment levels or categories) deviate from the overall mean of the entire dataset. It's calculated by comparing the variability among group means to the variability within each group.\\n\\n2. **Within-Groups Variance (Residual Variance):** This component of variance accounts for the variability within each group or treatment level. It represents the unexplained variability that remains after accounting for the differences among group means. It measures the variation of individual data points around their respective group means.\\n\\nUnderstanding the partitioning of variance is important for several reasons:\\n\\n1. **Identifying Significant Effects:** By partitioning the total variance into between-groups and within-groups components, ANOVA helps researchers determine whether the observed differences among group means are statistically significant or if they could have occurred by random chance.\\n\\n2. **Determining the Importance of Factors:** Partitioning variance allows researchers to evaluate how much of the total variability can be attributed to the factors being studied. This helps assess the relative importance of different factors and their interactions.\\n\\n3. **Model Selection and Interpretation:** Partitioning of variance aids in model selection by helping researchers decide whether the factors being studied should be included in the model. It also guides the interpretation of results by providing context for the significance of differences among groups.\\n\\n4. **Validating Assumptions:** Understanding how variance is partitioned can help researchers validate assumptions of ANOVA, such as the homogeneity of variance and normality of residuals.\\n\\nOverall, partitioning of variance provides a structured framework for analyzing the sources of variability in a dataset and supports the statistical inference process, enabling researchers to make well-informed conclusions about the effects of factors and interactions under investigation.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#que 3\n",
    "\"\"\"Partitioning of variance is a fundamental concept in Analysis of Variance (ANOVA) that involves breaking down the total variability in a dataset into different components that can be attributed to different sources of variation. The goal of partitioning variance is to understand how much of the total variation in the dependent variable can be explained by the factors being studied and how much is due to random or unexplained variability. This breakdown provides insights into the contributions of different factors and interactions between them, helping researchers draw meaningful conclusions about the significance of those factors.\n",
    "\n",
    "In ANOVA, the total variance is partitioned into two main components:\n",
    "\n",
    "1. **Between-Groups Variance (Between-Treatments Variance):** This component of variance measures the variation among the group means. It quantifies how much the means of different groups (treatment levels or categories) deviate from the overall mean of the entire dataset. It's calculated by comparing the variability among group means to the variability within each group.\n",
    "\n",
    "2. **Within-Groups Variance (Residual Variance):** This component of variance accounts for the variability within each group or treatment level. It represents the unexplained variability that remains after accounting for the differences among group means. It measures the variation of individual data points around their respective group means.\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "1. **Identifying Significant Effects:** By partitioning the total variance into between-groups and within-groups components, ANOVA helps researchers determine whether the observed differences among group means are statistically significant or if they could have occurred by random chance.\n",
    "\n",
    "2. **Determining the Importance of Factors:** Partitioning variance allows researchers to evaluate how much of the total variability can be attributed to the factors being studied. This helps assess the relative importance of different factors and their interactions.\n",
    "\n",
    "3. **Model Selection and Interpretation:** Partitioning of variance aids in model selection by helping researchers decide whether the factors being studied should be included in the model. It also guides the interpretation of results by providing context for the significance of differences among groups.\n",
    "\n",
    "4. **Validating Assumptions:** Understanding how variance is partitioned can help researchers validate assumptions of ANOVA, such as the homogeneity of variance and normality of residuals.\n",
    "\n",
    "Overall, partitioning of variance provides a structured framework for analyzing the sources of variability in a dataset and supports the statistical inference process, enabling researchers to make well-informed conclusions about the effects of factors and interactions under investigation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6acdd5a-ab4f-46ea-af5d-9e405d1f39f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 1023.3333333333333\n",
      "Explained Sum of Squares (SSE): 944.9333333333332\n",
      "Residual Sum of Squares (SSR): 78.40000000000009\n"
     ]
    }
   ],
   "source": [
    "#que 4\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Data for each group\n",
    "group_data = [\n",
    "    np.array([15, 18, 20, 23, 22]),\n",
    "    np.array([28, 32, 30, 29, 33]),\n",
    "    np.array([40, 38, 42, 39, 36])\n",
    "]\n",
    "\n",
    "# Calculate the overall mean\n",
    "overall_mean = np.mean(np.concatenate(group_data))\n",
    "\n",
    "# Calculate the Total Sum of Squares (SST)\n",
    "sst = np.sum([(x - overall_mean)**2 for data in group_data for x in data])\n",
    "\n",
    "# Calculate the Explained Sum of Squares (SSE)\n",
    "group_means = [np.mean(data) for data in group_data]\n",
    "sse = np.sum([len(data) * (mean - overall_mean)**2 for data, mean in zip(group_data, group_means)])\n",
    "\n",
    "# Calculate the Residual Sum of Squares (SSR)\n",
    "ssr = sst - sse\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e191f65-1275-46de-b427-0060f9253981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect A: [-2.11111111  2.22222222 -0.11111111]\n",
      "Main Effect B: [-1.11111111  1.22222222 -0.11111111]\n",
      "Interaction Effect: [[ 0.11111111 -0.22222222  0.11111111]\n",
      " [-0.22222222  0.44444444 -0.22222222]\n",
      " [ 0.11111111 -0.22222222  0.11111111]]\n"
     ]
    }
   ],
   "source": [
    "#que 5\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Simulated data organized as a 2D array (rows: levels of factor A, columns: levels of factor B)\n",
    "data = np.array([\n",
    "    [8, 10, 9],\n",
    "    [12, 15, 13],\n",
    "    [10, 12, 11]\n",
    "])\n",
    "\n",
    "# Calculate the grand mean (overall mean)\n",
    "grand_mean = np.mean(data)\n",
    "\n",
    "# Calculate the main effects for each factor\n",
    "main_effect_A = np.mean(data, axis=1) - grand_mean\n",
    "main_effect_B = np.mean(data, axis=0) - grand_mean\n",
    "\n",
    "# Calculate the interaction effect\n",
    "interaction_effect = data - (np.outer(main_effect_A, np.ones(data.shape[1])) + np.outer(np.ones(data.shape[0]), main_effect_B) + grand_mean)\n",
    "\n",
    "print(\"Main Effect A:\", main_effect_A)\n",
    "print(\"Main Effect B:\", main_effect_B)\n",
    "print(\"Interaction Effect:\", interaction_effect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9bf422f-7559-4913-88cd-63bfb42bf9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the context of a one-way ANOVA, the F-statistic and p-value provide essential information about whether there are significant differences between the groups' means. Let's interpret the results based on the F-statistic of 5.23 and a p-value of 0.02.\\n\\n1. **F-Statistic: 5.23**\\n   The F-statistic is a ratio of the variance between the group means to the variance within the groups. In this case, an F-statistic of 5.23 indicates that the between-group variability is larger compared to the within-group variability. This suggests that there might be differences in at least some of the group means.\\n\\n2. **P-Value: 0.02**\\n   The p-value associated with the F-statistic indicates the probability of obtaining such an extreme F-statistic value (or more extreme) if the null hypothesis is true. In this case, with a p-value of 0.02, the probability of observing an F-statistic as extreme as 5.23 under the assumption of no real differences between the groups is 0.02.\\n\\nBased on these results:\\n\\n- **Conclusion:** Since the p-value (0.02) is less than the chosen significance level (usually denoted as α, e.g., 0.05), you would reject the null hypothesis.\\n\\n- **Interpretation:** This implies that there are significant differences between at least some of the group means. In other words, you have evidence to suggest that the population means of the groups are not all equal.\\n\\n- **Further Analysis:** After rejecting the null hypothesis, you might want to perform post hoc tests (such as Tukey's HSD test) to identify which specific group(s) have significantly different means from each other.\\n\\nRemember that the interpretation should always be contextualized within the research question and the nature of the data. The F-statistic and p-value provide valuable insights, but it's also important to consider the effect size, practical significance, and domain knowledge when interpreting the results of an ANOVA.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#que 6\n",
    "\"\"\"In the context of a one-way ANOVA, the F-statistic and p-value provide essential information about whether there are significant differences between the groups' means. Let's interpret the results based on the F-statistic of 5.23 and a p-value of 0.02.\n",
    "\n",
    "1. **F-Statistic: 5.23**\n",
    "   The F-statistic is a ratio of the variance between the group means to the variance within the groups. In this case, an F-statistic of 5.23 indicates that the between-group variability is larger compared to the within-group variability. This suggests that there might be differences in at least some of the group means.\n",
    "\n",
    "2. **P-Value: 0.02**\n",
    "   The p-value associated with the F-statistic indicates the probability of obtaining such an extreme F-statistic value (or more extreme) if the null hypothesis is true. In this case, with a p-value of 0.02, the probability of observing an F-statistic as extreme as 5.23 under the assumption of no real differences between the groups is 0.02.\n",
    "\n",
    "Based on these results:\n",
    "\n",
    "- **Conclusion:** Since the p-value (0.02) is less than the chosen significance level (usually denoted as α, e.g., 0.05), you would reject the null hypothesis.\n",
    "\n",
    "- **Interpretation:** This implies that there are significant differences between at least some of the group means. In other words, you have evidence to suggest that the population means of the groups are not all equal.\n",
    "\n",
    "- **Further Analysis:** After rejecting the null hypothesis, you might want to perform post hoc tests (such as Tukey's HSD test) to identify which specific group(s) have significantly different means from each other.\n",
    "\n",
    "Remember that the interpretation should always be contextualized within the research question and the nature of the data. The F-statistic and p-value provide valuable insights, but it's also important to consider the effect size, practical significance, and domain knowledge when interpreting the results of an ANOVA.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1cb93-49a1-48d5-9cb9-b534834d4aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#que 7\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
